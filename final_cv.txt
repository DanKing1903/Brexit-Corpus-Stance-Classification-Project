Using TensorFlow backend.
Namespace(domain='all', file_path=None, model_type='all', save_model=False)
['logistic_regression', 'fast_text', 'mlp']
['logistic_regression', 'fast_text', 'mlp']
Fold  1
multi-class
Loading Data
Training Model
Fold  2
multi-class
Loading Data
Training Model
Fold  3
multi-class
Loading Data
Training Model
Fold  4
multi-class
Loading Data
Training Model
Fold  5
multi-class
Loading Data
Training Model
time elapsed: 23.061617222992936


Epochs: 1
/home/dan/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/dan/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

Hamming Loss:                 0.355


Micro-f1 score:               0.645
Macro-f1 score:               0.783

Accuracy                      0.645
['LR', 'Multi-class', 1, 0.7831130492189871, 0.6450491027271443, 0.6450491027271443]
Fold  1
multi-label
Loading Data
Training Model
Fold  2
multi-label
Loading Data
Training Model
Fold  3
multi-label
Loading Data
Training Model
Fold  4
multi-label
Loading Data
Training Model
Fold  5
multi-label
Loading Data
Training Model
time elapsed: 13.184627578986692


Epochs: 1

Hamming Loss:                 0.060

f1 scores 
 -----------
Agreement/disagreement:       0.792        13
Certainty:                    0.841        29
Contrariety:                  0.823       109
Hypotheticality:              0.796        48
Necessity:                    0.757        56
Prediction:                   0.839        88
Source of knowledge:          0.756        95
Tact/rudeness:                0.779        24
Uncertainty:                  0.859        83
Volition:                     0.791        13

Micro-f1 score:               0.810
Macro-f1 score:               0.803

Accuracy                      0.578
['LR', 'Multi-label', 1, 0.8033894662811203, 0.8103901272045435, 0.5784619188921859]
Fold  1
multi-class
multi-class
Loading Data
Training Model
vocab size 4537, sequence length 43
Building FastText Model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 43)                0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 43, 100)           453700    
_________________________________________________________________
global_average_pooling1d_1 ( (None, 100)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 637)               64337     
=================================================================
Total params: 518,037
Trainable params: 518,037
Non-trainable params: 0
_________________________________________________________________
None
2018-09-09 10:46:37.187322: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Fold  2
multi-class
multi-class
Loading Data
Training Model
vocab size 4561, sequence length 45
Building FastText Model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 45)                0         
_________________________________________________________________
embedding_2 (Embedding)      (None, 45, 100)           456100    
_________________________________________________________________
global_average_pooling1d_2 ( (None, 100)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 637)               64337     
=================================================================
Total params: 520,437
Trainable params: 520,437
Non-trainable params: 0
_________________________________________________________________
None
Fold  3
multi-class
multi-class
Loading Data
Training Model
vocab size 4569, sequence length 45
Building FastText Model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         (None, 45)                0         
_________________________________________________________________
embedding_3 (Embedding)      (None, 45, 100)           456900    
_________________________________________________________________
global_average_pooling1d_3 ( (None, 100)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 637)               64337     
=================================================================
Total params: 521,237
Trainable params: 521,237
Non-trainable params: 0
_________________________________________________________________
None
Fold  4
multi-class
multi-class
Loading Data
Training Model
vocab size 4518, sequence length 45
Building FastText Model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_4 (InputLayer)         (None, 45)                0         
_________________________________________________________________
embedding_4 (Embedding)      (None, 45, 100)           451800    
_________________________________________________________________
global_average_pooling1d_4 ( (None, 100)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 637)               64337     
=================================================================
Total params: 516,137
Trainable params: 516,137
Non-trainable params: 0
_________________________________________________________________
None
Fold  5
multi-class
multi-class
Loading Data
Training Model
vocab size 4426, sequence length 45
Building FastText Model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_5 (InputLayer)         (None, 45)                0         
_________________________________________________________________
embedding_5 (Embedding)      (None, 45, 100)           442600    
_________________________________________________________________
global_average_pooling1d_5 ( (None, 100)               0         
_________________________________________________________________
dense_5 (Dense)              (None, 637)               64337     
=================================================================
Total params: 506,937
Trainable params: 506,937
Non-trainable params: 0
_________________________________________________________________
None
time elapsed: 356.3923021429946


Epochs: 1
/home/dan/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/dan/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

Hamming Loss:                 0.359


Micro-f1 score:               0.641
Macro-f1 score:               0.749

Accuracy                      0.641
['FastText', 'Multi-class', 1, 0.7485974010228025, 0.6408877349159248, 0.6408877349159248]
Fold  1
multi-label
multi-label
Loading Data
Training Model
vocab size 4537, sequence length 43
Building FastText Model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_6 (InputLayer)         (None, 43)                0         
_________________________________________________________________
embedding_6 (Embedding)      (None, 43, 100)           453700    
_________________________________________________________________
global_average_pooling1d_6 ( (None, 100)               0         
_________________________________________________________________
dense_6 (Dense)              (None, 10)                1010      
=================================================================
Total params: 454,710
Trainable params: 454,710
Non-trainable params: 0
_________________________________________________________________
None
Fold  2
multi-label
multi-label
Loading Data
Training Model
vocab size 4561, sequence length 45
Building FastText Model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_7 (InputLayer)         (None, 45)                0         
_________________________________________________________________
embedding_7 (Embedding)      (None, 45, 100)           456100    
_________________________________________________________________
global_average_pooling1d_7 ( (None, 100)               0         
_________________________________________________________________
dense_7 (Dense)              (None, 10)                1010      
=================================================================
Total params: 457,110
Trainable params: 457,110
Non-trainable params: 0
_________________________________________________________________
None
Fold  3
multi-label
multi-label
Loading Data
Training Model
vocab size 4569, sequence length 45
Building FastText Model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_8 (InputLayer)         (None, 45)                0         
_________________________________________________________________
embedding_8 (Embedding)      (None, 45, 100)           456900    
_________________________________________________________________
global_average_pooling1d_8 ( (None, 100)               0         
_________________________________________________________________
dense_8 (Dense)              (None, 10)                1010      
=================================================================
Total params: 457,910
Trainable params: 457,910
Non-trainable params: 0
_________________________________________________________________
None
Fold  4
multi-label
multi-label
Loading Data
Training Model
vocab size 4518, sequence length 45
Building FastText Model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_9 (InputLayer)         (None, 45)                0         
_________________________________________________________________
embedding_9 (Embedding)      (None, 45, 100)           451800    
_________________________________________________________________
global_average_pooling1d_9 ( (None, 100)               0         
_________________________________________________________________
dense_9 (Dense)              (None, 10)                1010      
=================================================================
Total params: 452,810
Trainable params: 452,810
Non-trainable params: 0
_________________________________________________________________
None
Fold  5
multi-label
multi-label
Loading Data
Training Model
vocab size 4426, sequence length 45
Building FastText Model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_10 (InputLayer)        (None, 45)                0         
_________________________________________________________________
embedding_10 (Embedding)     (None, 45, 100)           442600    
_________________________________________________________________
global_average_pooling1d_10  (None, 100)               0         
_________________________________________________________________
dense_10 (Dense)             (None, 10)                1010      
=================================================================
Total params: 443,610
Trainable params: 443,610
Non-trainable params: 0
_________________________________________________________________
None
time elapsed: 260.77403763399343


Epochs: 1

Hamming Loss:                 0.062

f1 scores 
 -----------
Agreement/disagreement:       0.819        13
Certainty:                    0.848        29
Contrariety:                  0.800       109
Hypotheticality:              0.756        48
Necessity:                    0.740        56
Prediction:                   0.841        88
Source of knowledge:          0.732        95
Tact/rudeness:                0.801        24
Uncertainty:                  0.874        83
Volition:                     0.777        13

Micro-f1 score:               0.800
Macro-f1 score:               0.799

Accuracy                      0.548
['FastText', 'Multi-label', 1, 0.7986869504544407, 0.8003559905566882, 0.5481312703122792]
Fold  1
multi-task
multi-task
Loading Data
Training Model
vocab size 4537, sequence length 43
Building FastText Model
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 43)           0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 43, 100)      453700      input_11[0][0]                   
__________________________________________________________________________________________________
global_average_pooling1d_11 (Gl (None, 100)          0           embedding_11[0][0]               
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 1)            101         global_average_pooling1d_11[0][0]
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 1)            101         global_average_pooling1d_11[0][0]
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 1)            101         global_average_pooling1d_11[0][0]
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 1)            101         global_average_pooling1d_11[0][0]
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 1)            101         global_average_pooling1d_11[0][0]
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 1)            101         global_average_pooling1d_11[0][0]
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 1)            101         global_average_pooling1d_11[0][0]
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 1)            101         global_average_pooling1d_11[0][0]
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 1)            101         global_average_pooling1d_11[0][0]
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 1)            101         global_average_pooling1d_11[0][0]
==================================================================================================
Total params: 454,710
Trainable params: 454,710
Non-trainable params: 0
__________________________________________________________________________________________________
None
stacking
Fold  2
multi-task
multi-task
Loading Data
Training Model
vocab size 4561, sequence length 45
Building FastText Model
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_12 (InputLayer)           (None, 45)           0                                            
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 45, 100)      456100      input_12[0][0]                   
__________________________________________________________________________________________________
global_average_pooling1d_12 (Gl (None, 100)          0           embedding_12[0][0]               
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 1)            101         global_average_pooling1d_12[0][0]
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 1)            101         global_average_pooling1d_12[0][0]
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 1)            101         global_average_pooling1d_12[0][0]
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 1)            101         global_average_pooling1d_12[0][0]
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 1)            101         global_average_pooling1d_12[0][0]
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 1)            101         global_average_pooling1d_12[0][0]
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 1)            101         global_average_pooling1d_12[0][0]
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 1)            101         global_average_pooling1d_12[0][0]
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 1)            101         global_average_pooling1d_12[0][0]
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 1)            101         global_average_pooling1d_12[0][0]
==================================================================================================
Total params: 457,110
Trainable params: 457,110
Non-trainable params: 0
__________________________________________________________________________________________________
None
stacking
Fold  3
multi-task
multi-task
Loading Data
Training Model
vocab size 4569, sequence length 45
Building FastText Model
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 45)           0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 45, 100)      456900      input_13[0][0]                   
__________________________________________________________________________________________________
global_average_pooling1d_13 (Gl (None, 100)          0           embedding_13[0][0]               
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 1)            101         global_average_pooling1d_13[0][0]
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 1)            101         global_average_pooling1d_13[0][0]
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 1)            101         global_average_pooling1d_13[0][0]
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 1)            101         global_average_pooling1d_13[0][0]
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 1)            101         global_average_pooling1d_13[0][0]
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 1)            101         global_average_pooling1d_13[0][0]
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 1)            101         global_average_pooling1d_13[0][0]
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 1)            101         global_average_pooling1d_13[0][0]
__________________________________________________________________________________________________
dense_39 (Dense)                (None, 1)            101         global_average_pooling1d_13[0][0]
__________________________________________________________________________________________________
dense_40 (Dense)                (None, 1)            101         global_average_pooling1d_13[0][0]
==================================================================================================
Total params: 457,910
Trainable params: 457,910
Non-trainable params: 0
__________________________________________________________________________________________________
None
stacking
Fold  4
multi-task
multi-task
Loading Data
Training Model
vocab size 4518, sequence length 45
Building FastText Model
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_14 (InputLayer)           (None, 45)           0                                            
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 45, 100)      451800      input_14[0][0]                   
__________________________________________________________________________________________________
global_average_pooling1d_14 (Gl (None, 100)          0           embedding_14[0][0]               
__________________________________________________________________________________________________
dense_41 (Dense)                (None, 1)            101         global_average_pooling1d_14[0][0]
__________________________________________________________________________________________________
dense_42 (Dense)                (None, 1)            101         global_average_pooling1d_14[0][0]
__________________________________________________________________________________________________
dense_43 (Dense)                (None, 1)            101         global_average_pooling1d_14[0][0]
__________________________________________________________________________________________________
dense_44 (Dense)                (None, 1)            101         global_average_pooling1d_14[0][0]
__________________________________________________________________________________________________
dense_45 (Dense)                (None, 1)            101         global_average_pooling1d_14[0][0]
__________________________________________________________________________________________________
dense_46 (Dense)                (None, 1)            101         global_average_pooling1d_14[0][0]
__________________________________________________________________________________________________
dense_47 (Dense)                (None, 1)            101         global_average_pooling1d_14[0][0]
__________________________________________________________________________________________________
dense_48 (Dense)                (None, 1)            101         global_average_pooling1d_14[0][0]
__________________________________________________________________________________________________
dense_49 (Dense)                (None, 1)            101         global_average_pooling1d_14[0][0]
__________________________________________________________________________________________________
dense_50 (Dense)                (None, 1)            101         global_average_pooling1d_14[0][0]
==================================================================================================
Total params: 452,810
Trainable params: 452,810
Non-trainable params: 0
__________________________________________________________________________________________________
None
stacking
Fold  5
multi-task
multi-task
Loading Data
Training Model
vocab size 4426, sequence length 45
Building FastText Model
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 45)           0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 45, 100)      442600      input_15[0][0]                   
__________________________________________________________________________________________________
global_average_pooling1d_15 (Gl (None, 100)          0           embedding_15[0][0]               
__________________________________________________________________________________________________
dense_51 (Dense)                (None, 1)            101         global_average_pooling1d_15[0][0]
__________________________________________________________________________________________________
dense_52 (Dense)                (None, 1)            101         global_average_pooling1d_15[0][0]
__________________________________________________________________________________________________
dense_53 (Dense)                (None, 1)            101         global_average_pooling1d_15[0][0]
__________________________________________________________________________________________________
dense_54 (Dense)                (None, 1)            101         global_average_pooling1d_15[0][0]
__________________________________________________________________________________________________
dense_55 (Dense)                (None, 1)            101         global_average_pooling1d_15[0][0]
__________________________________________________________________________________________________
dense_56 (Dense)                (None, 1)            101         global_average_pooling1d_15[0][0]
__________________________________________________________________________________________________
dense_57 (Dense)                (None, 1)            101         global_average_pooling1d_15[0][0]
__________________________________________________________________________________________________
dense_58 (Dense)                (None, 1)            101         global_average_pooling1d_15[0][0]
__________________________________________________________________________________________________
dense_59 (Dense)                (None, 1)            101         global_average_pooling1d_15[0][0]
__________________________________________________________________________________________________
dense_60 (Dense)                (None, 1)            101         global_average_pooling1d_15[0][0]
==================================================================================================
Total params: 443,610
Trainable params: 443,610
Non-trainable params: 0
__________________________________________________________________________________________________
None
stacking
time elapsed: 386.70464970500325


Epochs: 1

Hamming Loss:                 0.062

f1 scores 
 -----------
Agreement/disagreement:       0.819        13
Certainty:                    0.849        29
Contrariety:                  0.798       109
Hypotheticality:              0.762        48
Necessity:                    0.746        56
Prediction:                   0.838        88
Source of knowledge:          0.727        95
Tact/rudeness:                0.801        24
Uncertainty:                  0.869        83
Volition:                     0.777        13

Micro-f1 score:               0.799
Macro-f1 score:               0.798

Accuracy                      0.545
['FastText', 'Multi-task', 1, 0.7984984361198671, 0.799304780642018, 0.5445598417408506]
Fold  1
0
multi-class
Loading Data
Training Model
Building Multilayer Perceptron
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_16 (InputLayer)        (None, 18886)             0         
_________________________________________________________________
dense_61 (Dense)             (None, 25)                472175    
_________________________________________________________________
dropout_1 (Dropout)          (None, 25)                0         
_________________________________________________________________
dense_62 (Dense)             (None, 25)                650       
_________________________________________________________________
dropout_2 (Dropout)          (None, 25)                0         
_________________________________________________________________
dense_63 (Dense)             (None, 637)               16562     
=================================================================
Total params: 489,387
Trainable params: 489,387
Non-trainable params: 0
_________________________________________________________________
None
Fold  2
0
multi-class
Loading Data
Training Model
Building Multilayer Perceptron
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_17 (InputLayer)        (None, 18908)             0         
_________________________________________________________________
dense_64 (Dense)             (None, 25)                472725    
_________________________________________________________________
dropout_3 (Dropout)          (None, 25)                0         
_________________________________________________________________
dense_65 (Dense)             (None, 25)                650       
_________________________________________________________________
dropout_4 (Dropout)          (None, 25)                0         
_________________________________________________________________
dense_66 (Dense)             (None, 637)               16562     
=================================================================
Total params: 489,937
Trainable params: 489,937
Non-trainable params: 0
_________________________________________________________________
None
Fold  3
0
multi-class
Loading Data
Training Model
Building Multilayer Perceptron
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_18 (InputLayer)        (None, 19048)             0         
_________________________________________________________________
dense_67 (Dense)             (None, 25)                476225    
_________________________________________________________________
dropout_5 (Dropout)          (None, 25)                0         
_________________________________________________________________
dense_68 (Dense)             (None, 25)                650       
_________________________________________________________________
dropout_6 (Dropout)          (None, 25)                0         
_________________________________________________________________
dense_69 (Dense)             (None, 637)               16562     
=================================================================
Total params: 493,437
Trainable params: 493,437
Non-trainable params: 0
_________________________________________________________________
None
Fold  4
0
multi-class
Loading Data
Training Model
Building Multilayer Perceptron
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_19 (InputLayer)        (None, 18673)             0         
_________________________________________________________________
dense_70 (Dense)             (None, 25)                466850    
_________________________________________________________________
dropout_7 (Dropout)          (None, 25)                0         
_________________________________________________________________
dense_71 (Dense)             (None, 25)                650       
_________________________________________________________________
dropout_8 (Dropout)          (None, 25)                0         
_________________________________________________________________
dense_72 (Dense)             (None, 637)               16562     
=================================================================
Total params: 484,062
Trainable params: 484,062
Non-trainable params: 0
_________________________________________________________________
None
Fold  5
0
multi-class
Loading Data
Training Model
Building Multilayer Perceptron
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_20 (InputLayer)        (None, 18390)             0         
_________________________________________________________________
dense_73 (Dense)             (None, 25)                459775    
_________________________________________________________________
dropout_9 (Dropout)          (None, 25)                0         
_________________________________________________________________
dense_74 (Dense)             (None, 25)                650       
_________________________________________________________________
dropout_10 (Dropout)         (None, 25)                0         
_________________________________________________________________
dense_75 (Dense)             (None, 637)               16562     
=================================================================
Total params: 476,987
Trainable params: 476,987
Non-trainable params: 0
_________________________________________________________________
None
time elapsed: 1131.8440073179954


Epochs: 1
/home/dan/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/dan/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

Hamming Loss:                 0.436


Micro-f1 score:               0.564
Macro-f1 score:               0.729

Accuracy                      0.564
['MLP', 'Multi-class', 1, 0.7292360234701413, 0.5635933305072771, 0.5635933305072771]
Fold  1
0
multi-label
Loading Data
Training Model
Building Multilayer Perceptron
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_21 (InputLayer)        (None, 18886)             0         
_________________________________________________________________
dense_76 (Dense)             (None, 25)                472175    
_________________________________________________________________
dropout_11 (Dropout)         (None, 25)                0         
_________________________________________________________________
dense_77 (Dense)             (None, 25)                650       
_________________________________________________________________
dropout_12 (Dropout)         (None, 25)                0         
_________________________________________________________________
dense_78 (Dense)             (None, 10)                260       
=================================================================
Total params: 473,085
Trainable params: 473,085
Non-trainable params: 0
_________________________________________________________________
None
Fold  2
0
multi-label
Loading Data
Training Model
Building Multilayer Perceptron
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_22 (InputLayer)        (None, 18908)             0         
_________________________________________________________________
dense_79 (Dense)             (None, 25)                472725    
_________________________________________________________________
dropout_13 (Dropout)         (None, 25)                0         
_________________________________________________________________
dense_80 (Dense)             (None, 25)                650       
_________________________________________________________________
dropout_14 (Dropout)         (None, 25)                0         
_________________________________________________________________
dense_81 (Dense)             (None, 10)                260       
=================================================================
Total params: 473,635
Trainable params: 473,635
Non-trainable params: 0
_________________________________________________________________
None
Fold  3
0
multi-label
Loading Data
Training Model
Building Multilayer Perceptron
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_23 (InputLayer)        (None, 19048)             0         
_________________________________________________________________
dense_82 (Dense)             (None, 25)                476225    
_________________________________________________________________
dropout_15 (Dropout)         (None, 25)                0         
_________________________________________________________________
dense_83 (Dense)             (None, 25)                650       
_________________________________________________________________
dropout_16 (Dropout)         (None, 25)                0         
_________________________________________________________________
dense_84 (Dense)             (None, 10)                260       
=================================================================
Total params: 477,135
Trainable params: 477,135
Non-trainable params: 0
_________________________________________________________________
None
Fold  4
0
multi-label
Loading Data
Training Model
Building Multilayer Perceptron
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_24 (InputLayer)        (None, 18673)             0         
_________________________________________________________________
dense_85 (Dense)             (None, 25)                466850    
_________________________________________________________________
dropout_17 (Dropout)         (None, 25)                0         
_________________________________________________________________
dense_86 (Dense)             (None, 25)                650       
_________________________________________________________________
dropout_18 (Dropout)         (None, 25)                0         
_________________________________________________________________
dense_87 (Dense)             (None, 10)                260       
=================================================================
Total params: 467,760
Trainable params: 467,760
Non-trainable params: 0
_________________________________________________________________
None
Fold  5
0
multi-label
Loading Data
Training Model
Building Multilayer Perceptron
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_25 (InputLayer)        (None, 18390)             0         
_________________________________________________________________
dense_88 (Dense)             (None, 25)                459775    
_________________________________________________________________
dropout_19 (Dropout)         (None, 25)                0         
_________________________________________________________________
dense_89 (Dense)             (None, 25)                650       
_________________________________________________________________
dropout_20 (Dropout)         (None, 25)                0         
_________________________________________________________________
dense_90 (Dense)             (None, 10)                260       
=================================================================
Total params: 460,685
Trainable params: 460,685
Non-trainable params: 0
_________________________________________________________________
None
time elapsed: 1023.2394460990035


Epochs: 1

Hamming Loss:                 0.118

f1 scores 
 -----------
Agreement/disagreement:       0.388        13
Certainty:                    0.538        29
Contrariety:                  0.620       109
Hypotheticality:              0.536        48
Necessity:                    0.511        56
Prediction:                   0.658        88
Source of knowledge:          0.609        95
Tact/rudeness:                0.351        24
Uncertainty:                  0.756        83
Volition:                     0.366        13

Micro-f1 score:               0.601
Macro-f1 score:               0.533

Accuracy                      0.353
['MLP', 'Multi-label', 1, 0.5333104522571406, 0.6006341968645506, 0.35253815175922004]
Fold  1
0
multi-task
Loading Data
Training Model
Building Multilayer Perceptron
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_26 (InputLayer)           (None, 18886)        0                                            
__________________________________________________________________________________________________
dense_91 (Dense)                (None, 25)           472175      input_26[0][0]                   
__________________________________________________________________________________________________
dropout_21 (Dropout)            (None, 25)           0           dense_91[0][0]                   
__________________________________________________________________________________________________
dense_92 (Dense)                (None, 25)           650         dropout_21[0][0]                 
__________________________________________________________________________________________________
dropout_22 (Dropout)            (None, 25)           0           dense_92[0][0]                   
__________________________________________________________________________________________________
dense_93 (Dense)                (None, 1)            26          dropout_22[0][0]                 
__________________________________________________________________________________________________
dense_94 (Dense)                (None, 1)            26          dropout_22[0][0]                 
__________________________________________________________________________________________________
dense_95 (Dense)                (None, 1)            26          dropout_22[0][0]                 
__________________________________________________________________________________________________
dense_96 (Dense)                (None, 1)            26          dropout_22[0][0]                 
__________________________________________________________________________________________________
dense_97 (Dense)                (None, 1)            26          dropout_22[0][0]                 
__________________________________________________________________________________________________
dense_98 (Dense)                (None, 1)            26          dropout_22[0][0]                 
__________________________________________________________________________________________________
dense_99 (Dense)                (None, 1)            26          dropout_22[0][0]                 
__________________________________________________________________________________________________
dense_100 (Dense)               (None, 1)            26          dropout_22[0][0]                 
__________________________________________________________________________________________________
dense_101 (Dense)               (None, 1)            26          dropout_22[0][0]                 
__________________________________________________________________________________________________
dense_102 (Dense)               (None, 1)            26          dropout_22[0][0]                 
==================================================================================================
Total params: 473,085
Trainable params: 473,085
Non-trainable params: 0
__________________________________________________________________________________________________
None
stacking
Fold  2
0
multi-task
Loading Data
Training Model
Building Multilayer Perceptron
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 18908)        0                                            
__________________________________________________________________________________________________
dense_103 (Dense)               (None, 25)           472725      input_27[0][0]                   
__________________________________________________________________________________________________
dropout_23 (Dropout)            (None, 25)           0           dense_103[0][0]                  
__________________________________________________________________________________________________
dense_104 (Dense)               (None, 25)           650         dropout_23[0][0]                 
__________________________________________________________________________________________________
dropout_24 (Dropout)            (None, 25)           0           dense_104[0][0]                  
__________________________________________________________________________________________________
dense_105 (Dense)               (None, 1)            26          dropout_24[0][0]                 
__________________________________________________________________________________________________
dense_106 (Dense)               (None, 1)            26          dropout_24[0][0]                 
__________________________________________________________________________________________________
dense_107 (Dense)               (None, 1)            26          dropout_24[0][0]                 
__________________________________________________________________________________________________
dense_108 (Dense)               (None, 1)            26          dropout_24[0][0]                 
__________________________________________________________________________________________________
dense_109 (Dense)               (None, 1)            26          dropout_24[0][0]                 
__________________________________________________________________________________________________
dense_110 (Dense)               (None, 1)            26          dropout_24[0][0]                 
__________________________________________________________________________________________________
dense_111 (Dense)               (None, 1)            26          dropout_24[0][0]                 
__________________________________________________________________________________________________
dense_112 (Dense)               (None, 1)            26          dropout_24[0][0]                 
__________________________________________________________________________________________________
dense_113 (Dense)               (None, 1)            26          dropout_24[0][0]                 
__________________________________________________________________________________________________
dense_114 (Dense)               (None, 1)            26          dropout_24[0][0]                 
==================================================================================================
Total params: 473,635
Trainable params: 473,635
Non-trainable params: 0
__________________________________________________________________________________________________
None
stacking
Fold  3
0
multi-task
Loading Data
Training Model
Building Multilayer Perceptron
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_28 (InputLayer)           (None, 19048)        0                                            
__________________________________________________________________________________________________
dense_115 (Dense)               (None, 25)           476225      input_28[0][0]                   
__________________________________________________________________________________________________
dropout_25 (Dropout)            (None, 25)           0           dense_115[0][0]                  
__________________________________________________________________________________________________
dense_116 (Dense)               (None, 25)           650         dropout_25[0][0]                 
__________________________________________________________________________________________________
dropout_26 (Dropout)            (None, 25)           0           dense_116[0][0]                  
__________________________________________________________________________________________________
dense_117 (Dense)               (None, 1)            26          dropout_26[0][0]                 
__________________________________________________________________________________________________
dense_118 (Dense)               (None, 1)            26          dropout_26[0][0]                 
__________________________________________________________________________________________________
dense_119 (Dense)               (None, 1)            26          dropout_26[0][0]                 
__________________________________________________________________________________________________
dense_120 (Dense)               (None, 1)            26          dropout_26[0][0]                 
__________________________________________________________________________________________________
dense_121 (Dense)               (None, 1)            26          dropout_26[0][0]                 
__________________________________________________________________________________________________
dense_122 (Dense)               (None, 1)            26          dropout_26[0][0]                 
__________________________________________________________________________________________________
dense_123 (Dense)               (None, 1)            26          dropout_26[0][0]                 
__________________________________________________________________________________________________
dense_124 (Dense)               (None, 1)            26          dropout_26[0][0]                 
__________________________________________________________________________________________________
dense_125 (Dense)               (None, 1)            26          dropout_26[0][0]                 
__________________________________________________________________________________________________
dense_126 (Dense)               (None, 1)            26          dropout_26[0][0]                 
==================================================================================================
Total params: 477,135
Trainable params: 477,135
Non-trainable params: 0
__________________________________________________________________________________________________
None
stacking
Fold  4
0
multi-task
Loading Data
Training Model
Building Multilayer Perceptron
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 18673)        0                                            
__________________________________________________________________________________________________
dense_127 (Dense)               (None, 25)           466850      input_29[0][0]                   
__________________________________________________________________________________________________
dropout_27 (Dropout)            (None, 25)           0           dense_127[0][0]                  
__________________________________________________________________________________________________
dense_128 (Dense)               (None, 25)           650         dropout_27[0][0]                 
__________________________________________________________________________________________________
dropout_28 (Dropout)            (None, 25)           0           dense_128[0][0]                  
__________________________________________________________________________________________________
dense_129 (Dense)               (None, 1)            26          dropout_28[0][0]                 
__________________________________________________________________________________________________
dense_130 (Dense)               (None, 1)            26          dropout_28[0][0]                 
__________________________________________________________________________________________________
dense_131 (Dense)               (None, 1)            26          dropout_28[0][0]                 
__________________________________________________________________________________________________
dense_132 (Dense)               (None, 1)            26          dropout_28[0][0]                 
__________________________________________________________________________________________________
dense_133 (Dense)               (None, 1)            26          dropout_28[0][0]                 
__________________________________________________________________________________________________
dense_134 (Dense)               (None, 1)            26          dropout_28[0][0]                 
__________________________________________________________________________________________________
dense_135 (Dense)               (None, 1)            26          dropout_28[0][0]                 
__________________________________________________________________________________________________
dense_136 (Dense)               (None, 1)            26          dropout_28[0][0]                 
__________________________________________________________________________________________________
dense_137 (Dense)               (None, 1)            26          dropout_28[0][0]                 
__________________________________________________________________________________________________
dense_138 (Dense)               (None, 1)            26          dropout_28[0][0]                 
==================================================================================================
Total params: 467,760
Trainable params: 467,760
Non-trainable params: 0
__________________________________________________________________________________________________
None
stacking
Fold  5
0
multi-task
Loading Data
Training Model
Building Multilayer Perceptron
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_30 (InputLayer)           (None, 18390)        0                                            
__________________________________________________________________________________________________
dense_139 (Dense)               (None, 25)           459775      input_30[0][0]                   
__________________________________________________________________________________________________
dropout_29 (Dropout)            (None, 25)           0           dense_139[0][0]                  
__________________________________________________________________________________________________
dense_140 (Dense)               (None, 25)           650         dropout_29[0][0]                 
__________________________________________________________________________________________________
dropout_30 (Dropout)            (None, 25)           0           dense_140[0][0]                  
__________________________________________________________________________________________________
dense_141 (Dense)               (None, 1)            26          dropout_30[0][0]                 
__________________________________________________________________________________________________
dense_142 (Dense)               (None, 1)            26          dropout_30[0][0]                 
__________________________________________________________________________________________________
dense_143 (Dense)               (None, 1)            26          dropout_30[0][0]                 
__________________________________________________________________________________________________
dense_144 (Dense)               (None, 1)            26          dropout_30[0][0]                 
__________________________________________________________________________________________________
dense_145 (Dense)               (None, 1)            26          dropout_30[0][0]                 
__________________________________________________________________________________________________
dense_146 (Dense)               (None, 1)            26          dropout_30[0][0]                 
__________________________________________________________________________________________________
dense_147 (Dense)               (None, 1)            26          dropout_30[0][0]                 
__________________________________________________________________________________________________
dense_148 (Dense)               (None, 1)            26          dropout_30[0][0]                 
__________________________________________________________________________________________________
dense_149 (Dense)               (None, 1)            26          dropout_30[0][0]                 
__________________________________________________________________________________________________
dense_150 (Dense)               (None, 1)            26          dropout_30[0][0]                 
==================================================================================================
Total params: 460,685
Trainable params: 460,685
Non-trainable params: 0
__________________________________________________________________________________________________
None
stacking
time elapsed: 1254.3808638710034


Epochs: 1

Hamming Loss:                 0.123

f1 scores 
 -----------
Agreement/disagreement:       0.346        13
Certainty:                    0.456        29
Contrariety:                  0.587       109
Hypotheticality:              0.488        48
Necessity:                    0.487        56
Prediction:                   0.647        88
Source of knowledge:          0.611        95
Tact/rudeness:                0.285        24
Uncertainty:                  0.707        83
Volition:                     0.298        13

Micro-f1 score:               0.578
Macro-f1 score:               0.491

Accuracy                      0.319
['MLP', 'Multi-task', 1, 0.49109726686922883, 0.5775587689812977, 0.3186290094672884]
                      Epochs  F1 Macro   F1 Micro       EMR
Model    Domain                                            
LR       Multi-class       1  0.783113   0.645049  0.645049
         Multi-label       1  0.803389   0.810390  0.578462
FastText Multi-class       1  0.748597   0.640888  0.640888
         Multi-label       1  0.798687   0.800356  0.548131
         Multi-task        1  0.798498   0.799305  0.544560
MLP      Multi-class       1  0.729236   0.563593  0.563593
         Multi-label       1  0.533310   0.600634  0.352538
         Multi-task        1  0.491097   0.577559  0.318629
