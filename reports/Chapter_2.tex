\documentclass[Dissertation.tex]{subfiles}
\begin{document}

\chapter{Background and Literature Review}
This chapter contains a review of previously written academic literature in topics related to the project aims and objectives. The chapter begins by examining topics in machine learning and text classification to provide context for later sections. Following this is an exploration of the meaning of stance both in everyday language and in academia to introduce the concept to readers with no formal education in linguistics. We then review existing research related to stance detection, and finally, the chapter concludes with a review of the Brexit Blog Corpus by Simaki et al.\ \cite{simakiAnnotatingSpeakerStance2017}, which is the principle dataset for this project.
 



\section{Machine Learning}


\begin{figure}
	\centering
	\includegraphics[width=10cm]{ML_hierachy.pdf}
	\caption{Hierarchy of machine learning methods}
	\label{mlHierachy}
\end{figure}

Before delving into further material concerning stance detection it is helpful to briefly review the topic of machine learning. Machine learning is the process of computationally inferring a solution to a problem using past experience \cite{alpaydinIntroductionMachineLearning2014}. We take some prior knowledge (a \textbf{dataset}), extract relevant information (the \textbf{features}) and define a programmatic goal to be achieved (the \textbf{objective function}). We then iteratively train a program to identify patterns in the data (by \textbf{optimization} of the objective function), leading to a\textbf{ model }of the dataset. The model can be used to make predictions in future data, or to provide descriptions of existing data\cite{alpaydinIntroductionMachineLearning2014}. There are many forms of machine learning (figure \ref{mlHierachy}), but we can group them by the amount of `supervision' they receive in training \cite{geronHandsonMachineLearning2017}. All models in this project were obtained using supervised learning methods, so this review concentrates on supervised learning and considers other categories such as unsupervised, semi-supervised and reinforcement learning out of scope.

\subsection{Supervised Learning}
In the domain of supervised learning, each instance within our dataset includes the desired output (the \textbf{target variable}) \cite{geronHandsonMachineLearning2017}. In formal notation, for supervised learning we require that a dataset $ \mathcal{D}$ of size $N$ is specified by $ \mathcal{D} = \{(\mathbf{x}_i, y_i) | i = 1:N\}$, where $ \mathbf{x}_i \in \mathcal{X} $ represents an instance in the input space $ \mathcal{X} $ and $ y_i \in \mathcal{Y}$ is the associated target variable in the target space $ \mathcal{Y} $. We seek to learn a classifier $ \gamma $ that maps inputs to outputs:
$$ \gamma: \mathcal{X}\rightarrow\mathcal{Y}$$ 

The learning process is `supervised' during training by evaluating the quality of model output against an exact solution: correct predictions are rewarded and incorrect predictions are penalised via an optimization process such as stochastic gradient descent \cite{mendelsonAdvancedLecturesMachine2003}. Supervised learning includes both numeric prediction (regression), where the target variable is continuous, and classification, where the target variable is categorical \cite{wittenDataMiningPractical2011}. Since this project concerns a text classification task, this review is focussed on classification, with other kinds of supervised learning considered out of scope. 


\begin{figure}[h]
	\centering
	\includegraphics[width=4in]{word_embeddings.pdf}
	\caption{Visualisation of word embeddings in 2-D
		\label{fig:wordEmbeds} space}
\end{figure}

Section \ref{sec:vecSpace} briefly introduced the vector space model and highlights the importance of feature extraction for textual data. While simple N-gram bag of words models are conceptually simple and generally effective, such approaches suffer from two important problems. Firstly, words are treated as atomic units and no semantic similarity is captured in the feature representation. Secondly, when using large training vocabularies feature vectors become very sparse: in an N-gram model feature dimensionality scales as $ |V|^N $ where $ N $ is gram length and $ |V|  $ is the size of vocabulary.  The complete Oxford English Dictionary alone contains nearly 250,000 words \cite{HowManyWords} without taking account of technical terms, dialects and regional spelling variation, so it is clear that any suitably large training corpus will result in sparse representations \cite{jurafskySpeechLanguageProcessing}. The limitations of such sparse feature representations can however be avoided by using vector semantics and dense word embeddings \cite{jurafskySpeechLanguageProcessing}.

The distributional hypothesis  posits that words appearing in similar contexts have similar meanings \cite{sahlgren2008distributional}. Vector semantics aims to capture this similarity by representing words as dense vectors embedded in a multidimensional space, which are learned from the distributions of words and contexts in corpora \cite{jurafskySpeechLanguageProcessing}. This is known as representation learning, where we generate representations of text by unsupervised learning instead of feature engineering \cite{jurafskySpeechLanguageProcessing}. Figure \ref{fig:wordEmbeds} shows an extract of what a word vector embedding space might look like projected into two dimensions. Notice that semantically similar words are spatially grouped. There are several popular formats for word embeddings including skip-gram (word2vec) \cite{mikolovEfficientEstimationWord2013}, GloVe \cite{penningtonGloveGlobalVectors2014} and fastText \cite{joulinBagTricksEfficient2016}, though there are broad similarities between all three methods. A key innovation in this field was the use of running text as implicitly supervised training data - any given sentence already contains the target variable for a classifier that predicts a word in context \cite{jurafskySpeechLanguageProcessing}. In particular, a neural language model that accomplishes this will learn a weight matrix from which embedded word representations can be extracted \cite{jurafskySpeechLanguageProcessing}. 

\section{Classifers}
This section reviews relevant material concerning the classifiers used throughout this project. Classifier choice can play a significant role in model performance, so it is important to ensure the choice is appropriate for the task at hand.
\subsection{Logistic Regression}

\begin{figure}
	\centering
	\includegraphics[width=3in]{sigmoid.pdf}
	\caption{Logistic sigmoid function}
	\label{logisticFunction}
\end{figure}
Logistic regression is part of a family of classifiers known as linear classifiers.	Bishop \cite{bishopPatternRecognitionMachine2006} provides a comprehensive overview of linear classifiers and defines the simplest general form as follows:

$$ 
y(\mathbf{x}) = f(\mathbf{w}^\top\mathbf{x}) 
$$

Here $ y $ is the classifier prediction, $ \mathbf{x} $ is the input data vector, $ \mathbf{w} $ is a learned parameter vector and $ f(\cdot) $ is an \textit{activation function} - a nonlinear transformation that maps unbounded real numbers to probabilities in the range \{0,1\}. Specifically in logistic regression we introduce the \textit{logistic sigmoid} activation function, defined in  \cite{bishopPatternRecognitionMachine2006} as:

\[ \sigma(a) = \frac{1}{1 + \mathrm{exp}(-a)} \]

Therefore, with the inclusion of a feature function $ \phi(\cdot) $ binary logistic regression with classes $ \mathbb{C} = \{c_1, c_2\} $ \cite{bishopPatternRecognitionMachine2006} defines the probability of a positive label as:

\[ \mathrm{P}(c_1|\phi(\mathbf{x})) = y(\phi(\mathbf{x})) =  \sigma(\mathbf{w}^\top\phi(\mathbf{x}
)) \]

It follows that the probability of the negative label is given by: $$ \mathrm{P}(c_2| \phi(\mathbf{x})) = 1-\mathrm{P}(c_1|\phi(\mathbf{x})) $$

Logistic regression can also be generalized to multi-class domains where $ \mathbb{C} = \{c_1,\ c_2\, \ \dots,\ c_j \} $:

\[ \mathrm{P}(c_k|\phi(\mathbf{x})) = y_k(\phi(\mathbf{x})) =  \frac{\mathrm{exp}(a_k)}{\sum_{j}\mathrm{exp}(a_j)}   \]

Where the activation $ a_k $ is given by:
\[a_k =  \mathbf{w}_k^\top\phi(\mathbf{x})  \]

In order to use logistic regression in a multi-label context the binary relevance transformation described in section \ref{sec:multiLabel} must be used and multiple binary logistic regression classifiers thereby trained.

\subsection{Neural Networks}
\begin{figure}[h]
	
	\centering
	\includegraphics[width=5in]{neuron.pdf}
	
	\caption{Artificial neuron}
	\label{fig:neuron}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=5in]{simple_NN.pdf}
	\caption{Simple multilayer perceptron}
	\label{fig:multilayerPercep}
\end{figure}

In recent years there has been a resurgence in interest amongst researchers about neural networks and deep learning. First investigated in the 1950s, current technological advancements have lead to enourmous growth in performance and scalability of neural networks \cite{pattersonDeepLearningPractitioner2017}. In particular, neural networks have shown great promise and versatility in many natural language processing tasks. This section reviews the basic elements to neural networks and certain architectures that are used in the approach to stance detection presented by this project. 

The basic unit of a neural network is the artificial neuron (see Figure \ref{fig:neuron}). Each neuron maps a vector of inputs to a single output by a non linear activation function $ h(\cdot) $ \cite{pattersonDeepLearningPractitioner2017}. The activation $ a_k $ of a neuron receiving inputs $ \mathbf{x} = \{x_1,x_2,\ \dots,\ x_n\} $ is defined by \cite{pattersonDeepLearningPractitioner2017}:

\[ a_k = h(b + \sum_{i=1}^n w_ix_i) \]

Where $ b $ and $ \mathbf{w} = \{w_1,w_2, \ \dots, \ w_n\} $ are learned parameters. Common activation functions include the logistic sigmoid (see Figure \ref{logisticFunction}) and the rectified linear unit function (ReLU)(see Figure \ref{fig:ReLU}).

\begin{figure}
	\centering
	\includegraphics[width=3in]{ReLu.pdf}
	
	\caption{ReLU activation function}
	\label{fig:ReLU}
	
\end{figure}

Neural networks are structured in layers of neurons, the connectivity between and within which defines the architecture of a network. In the simplest networks (known as multilayer perceptrons, see Figure \ref{fig:multilayerPercep}) each neuron is fully connected, taking input from every unit in the preceding layer and feeding forward the output activation to every unit in the following layer. \cite{pattersonDeepLearningPractitioner2017}. More complex architectures introduce semi-connected layers (Convolutional Neural Network) and recurrent layers (Recurrent Neural Networks)\cite{pattersonDeepLearningPractitioner2017},\cite{jurafskySpeechLanguageProcessing}


\subsection{FastText}


FastText is an architecture for text classification and learning word embeddings (reviewed further in Section \ref{sec:textClass} and \ref{sec:featsEmbeds}) first proposed by Joulin et al. \cite{joulinBagTricksEfficient2016}. FastText is known to be simple and efficient, generally scoring on par with complex deep learning models while being orders of magnitude faster to train and test. Linear classifiers (such as logistic regression or SVMs) are known to be fast and efficient baselines when used with bag-of-words representation for sentence classification. However, in such classifiers parameters are not shared between features or classes, which can inhibit generalizability and scalability \cite{joulinBagTricksEfficient2016}. FastText offers a solution to this problem using rank constraint and word embeddings. Figure \ref{fig:fastTextArch} shows the fastText classifier. Words are mapped to a dense embedding representation, and then the word representations are averaged into a text representation, which in turn is passed into a linear classifier \cite{joulinBagTricksEfficient2016}. The word embeddings learned during this process may be reused. 

\begin{figure}[h]
	\centering
	\includegraphics[width=5in]{fastText.pdf}
	\caption{FastText embedding architecture for $ d $-dimensional embedding and sentence length $ N $}
	\label{fig:fastTextArch}
\end{figure}



\section{Text Classification}\label{sec:textClass}


\begin{figure}

	\centering
	\includegraphics[width=11cm]{Spam_classifier.pdf}
	\caption{Simple spam classifier flowchart}
	\label{spamClassifier}
\end{figure}
Text Classification is a supervised learning, classification task in which we seek to assign a label to a text \cite{jurafskySpeechLanguageProcessing}. It has much importance in the fields of Information Retrieval and other areas of Natural Language Processing  as highlighted ing Manning, Raghavan and Sch\"{u}tze \cite{manningIntroductionInformationRetrieval2009}.
An example of text classification is flagging unwanted spam emails \cite{geronHandsonMachineLearning2017}. The dataset might consist of a collection of emails, with each instance containing the text of an email, associated metadata and a label of `spam` or `not-spam' \cite{geronHandsonMachineLearning2017}. The objective would be to learn a model that can accurately classify new incoming emails accordingly as `spam' or `not-spam'(see figure \ref{spamClassifier}). 

\subsection{Formal Definition}
In text classification we are given a document $ d \in \mathcal{X}$ where $ \mathcal{X} $ is the input document space, and  a set of disjoint classes $ \mathbb{C} = \{c_1,c_2,\dots c_j\}$  \cite{manningIntroductionInformationRetrieval2009}. The document space is generally some form of high dimensional feature space, while the classes are usually pre-defined natural language labels such as `spam' and `not-spam' as in figure~\ref{spamClassifier}\cite{manningIntroductionInformationRetrieval2009}. We have a dataset $\mathcal{D}$ of labelled instances $(d,c) \in \mathcal{X}\times \mathbb{C}$ and we seek to learn a classification function $ \gamma $ that maps inputs to classes \cite{manningIntroductionInformationRetrieval2009}:

$$
\gamma: \mathcal{X}\rightarrow \mathbb{C}
$$

We denote the \textit{learning method} as $ \Gamma $ such that $ \Gamma(\mathcal{D})= \gamma $ \cite{manningIntroductionInformationRetrieval2009}. The learning method applied to the dataset returns the classifier.  If $ |\mathbb{C}| = 2 $, then the classifier is performing \textit{binary classification}, which is a special case of the more general domain of \textit{multi-class classification}, where $ |\mathbb{C}| > 2 $ \cite{manningIntroductionInformationRetrieval2009}.


\subsection{Multi-Label Classification}\label{sec:multiLabel}
In the previous sections we have considered \textit{single-label classification}, in which we assume  that for each instance there is only one correct label from the set of classes $ \mathbb{C} $. However, there are many situations in which this assumption does not hold, and we wish to assign to each instance a set of labels, $ \mathbf{y} = \{y_1,\ y_2,\  \dots, \ y_k\} \subseteq \mathbb{C}\} $ \cite{tsoumakasMiningMultilabelData2009}. This is known as \textit{multi-label classification}. For example, if we consider a hypothetical film genre classifier, we might want to label a film both as 'Action' and 'Thriller' or 'Romantic' and 'Comedy'. The Brexit Blog Corpus \cite{simakiAnnotatingSpeakerStance2017} provides a multi-label dataset, so it is important for the purpose of this project to review material concerning multi-label classification.

%\subsubsection{Problem Transformation Methods}
There exist a number of methods to transform multi-label datasets into forms that allow the application of traditional single-label classifying algorithms. Sorrower \cite{sorowerLiteratureSurveyAlgorithms2018} provides a useful review of such methods, some of which we will outline below. Certain methods such as \textit{select-max}, \textit{select-min} and \textit{select-random} extract only a single-label from each instance, while the \textit{ignore} method ignores all instances with more than one label \cite{sorowerLiteratureSurveyAlgorithms2018}. However, such methods do not provide fidelity with respect to the original dataset, and so are not considered further in this review. 

Of particular interest to this review are the \textit{copy}, \textit{label power set}, \textit{binary relevance} and \textit{ranking by pairwise comparison} methods \cite{sorowerLiteratureSurveyAlgorithms2018}. To illustrate these methods Sorower \cite{sorowerLiteratureSurveyAlgorithms2018} considers a small data set, where four instances each belong to at least one class from the set $\mathbb{C} = \{c_1, \ c_2, \ c_3, \ c_4\}$ (see Table \ref{miniData}, part (a)).

 In the copy transformation each instance $ (x_i, \mathbf{y}_i) $ is replaced with $ |\mathbf{y}_i| $ instances $ (x_i, c_j) $ for $ c_j \in \mathbf{y}_i $ (see Table \ref{miniData} part (b)) \cite{sorowerLiteratureSurveyAlgorithms2018}. The problem can now be attempted using a single-label multi-class classifier.

In the label power set transformation each unique set of labels in the data is considered a single class in the transformed data (see Table \ref{miniData} part (c)) \cite{sorowerLiteratureSurveyAlgorithms2018}. Thus, the problem can now be solved by a single-label multi-class classifier.

In the binary relevance method, $ k $ copies of the original dataset are created, where $ k = |\mathbb{C}| $. Each dataset is associated with one label $c_i \in \mathbb{C}$ with instances belonging to $ c_i $ labelled positively and all others labelled negatively (see Table \ref{miniData} part (d)). The target variable for each data set is therefore a binary variable, hence separate binary classifiers may be trained on each dataset and the results aggregated to provide the multi-label output \cite{sorowerLiteratureSurveyAlgorithms2018}.

Finally, in the ranking by pairwise comparison (RPC) method	the dataset is transformed into $\bigl( \begin{smallmatrix}k\\2\end{smallmatrix}\bigr)$  binary datasets. Each dataset corresponds to two labels, and retains the instances annotated as belonging to one of the labels but not both (see Table \ref{miniData} part (e) \cite{sorowerLiteratureSurveyAlgorithms2018}. Binary classifiers can be trained on each new dataset, and the votes from each aggregated to create a full multi-label classification.

 
		
\begin{table}[]
	\centering

	\caption{Sample dataset in multi-label and label power set formats}
	\label{miniData}
	\begin{subfigure}[t]{0.25\linewidth}
		\centering
		\subcaption{Original format}
		\begin{tabular*}{1\linewidth}{@{}cc@{}}
	
			\toprule
			Instance & Labels \\ \midrule
			1	&	$ \{c_2,c_3\} $        \\
			2	&	$ \{c_1\} $        \\
			3	&	$ \{c_1,c_2,c_3\} $        \\
			4	&	$ \{c_2, c_4\} $        \\ \bottomrule
		\end{tabular*}
	\end{subfigure}
	\qquad
	\begin{subfigure}[t]{0.25\linewidth}
		\centering
		\subcaption{Copy format}
		\begin{tabular*}{1\linewidth}{@{}cc@{}}
			
			\toprule
			Instance & Label \\ \midrule
			1a	&	$ c_2$      \\
			1b  &	$ c_3 $     \\
			2	&	$ c_1 $     \\
			3a	&	$ c_1$		\\
			3b	&	$ c_2$		\\
			3c	&	$ c_3$      \\
			4a	&	$ c_2$		\\
			4b 	&	$ c_4 $     \\ \bottomrule
		\end{tabular*}
	\end{subfigure}
\qquad
	\begin{subfigure}[t]{0.25\linewidth}
		\centering
		\subcaption{Label power set }
		\begin{tabular*}{1\linewidth}{@{}cc@{}}
			
			\toprule
			Instance & Label \\ \midrule
			1	&	$ c_{2,3} $        \\
			2	&	$ c_1$        \\
			3	&	$ c_{1,2,3} $        \\
			4	&	$ c_{2,4} $        \\ \bottomrule
		\end{tabular*}
	\end{subfigure}

\vspace{3.5em}

	\begin{subfigure}{1\linewidth}
		\centering
		\subcaption{Binary relevance format}			
			\begin{subfigure}{0.2\linewidth}
				\subcaption*{$ c_1 $}
				\begin{tabular*}{1\linewidth}{@{}cc@{}}
					\toprule
					Instance & Label \\ \midrule
					1	&	0        \\
					2	&	1        \\
					3	&   1 	    \\
					4	&   0	    \\ \bottomrule
				\end{tabular*}
			\end{subfigure}
			\quad
			\begin{subfigure}{0.2\linewidth}
				\subcaption*{$ c_2 $}
				\begin{tabular*}{1\linewidth}{@{}cc@{}}
					\toprule
					Instance & Label \\ \midrule
					1	&	1        \\
					2	&	0        \\
					3	&	1        \\
					4	&	1        \\ \bottomrule
				\end{tabular*}
			\end{subfigure}
			\quad
			\begin{subfigure}{0.2\linewidth}
				\subcaption*{$c_3$}
				\begin{tabular*}{1\linewidth}{@{}cc@{}}
					\toprule
					Instance & Label \\ \midrule
					1	&	1        \\
					2	&	0        \\
					3	&	1        \\
					4	&	1        \\ \bottomrule
				\end{tabular*}
			\end{subfigure}
			\quad
			\begin{subfigure}{0.2\linewidth}
				\subcaption*{$ c_4 $}
				\begin{tabular*}{1\linewidth}{@{}cc@{}}
					\toprule
					Instance & Label \\ \midrule
					1	&	0        \\
					2	&	0        \\
					3	&	0        \\
					4	&	1        \\ \bottomrule
				\end{tabular*}
			\end{subfigure}
		\end{subfigure}

\vspace{3.5em}
	\begin{subfigure}{1\linewidth}
	\centering
	\subcaption{Ranking by pairwise comparison format}			
	\begin{subfigure}[t]{0.2\linewidth}
		\subcaption*{$ c_1, c_2 $}
		\begin{tabular*}{1\linewidth}{@{}cc@{}}
			\toprule
			Instance & Label \\ \midrule
			1	&	$ c_{\neg1,2} $ 	       \\
			2	&	$ c_{1,\neg2} $        \\
			4	&   $ c_{\neg1,2} $	    \\ \bottomrule
		\end{tabular*}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.2\linewidth}
		\subcaption*{$ c_1,c_2 $}
		\begin{tabular*}{1\linewidth}{@{}cc@{}}
			\toprule
			Instance & Label \\ \midrule
			1	&	$ c_{\neg1,3} $ 	       \\
			2	&	$ c_{1,\neg3} $        \\ \bottomrule

		\end{tabular*}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.2\linewidth}
		\subcaption*{$c_1, c_4$}
		\begin{tabular*}{1\linewidth}{@{}cc@{}}
			\toprule
			Instance & Label \\ \midrule
			2	&	$ c_{1,\neg4} $        \\
			3	&	$ c_{1,\neg4} $        \\
			4	&   $ c_{\neg1,4} $	    \\ \bottomrule
		\end{tabular*}
	\end{subfigure}

	\vspace{1em}	
	\begin{subfigure}[t]{0.2\linewidth}
		\subcaption*{$ c_2, c_3$}
		\begin{tabular*}{1\linewidth}{@{}cc@{}}
			\toprule
			Instance & Label \\ \midrule
			4	&   $ c_{2,\neg3} $	    \\ \bottomrule
		\end{tabular*}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.2\linewidth}
		\subcaption*{$ c_2,c_4 $}
		\begin{tabular*}{1\linewidth}{@{}cc@{}}
			\toprule
			Instance & Label \\ \midrule
			1	&	$ c_{2,\neg4} $        \\
			3	&	$ c_{2,\neg4} $        \\
			\bottomrule
		\end{tabular*}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.2\linewidth}
		\subcaption*{$c_3,c_4$}
		\begin{tabular*}{1\linewidth}{@{}cc@{}}
			\toprule
			Instance & Label \\ \midrule
			1	&	$ c_{3,\neg4} $        \\
			3	&	$ c_{3,\neg4} $        \\
			4	&	$ c_{\neg3,4} $ \\
			\bottomrule
		\end{tabular*}
	\end{subfigure}	
\end{subfigure}
	
	
\end{table}


\subsection{Multi-Task Learning} \label{sec:multiTask}
\begin{figure}
	\centering
	\begin{subfigure}{0.5\linewidth}
		\centering
		\hspace*{2em}
		\includegraphics[height=2in]{hard_param_sharing.pdf}
		\subcaption{Hard parameter sharing}
	\end{subfigure}\begin{subfigure}{0.5\linewidth}
	\centering
			\hspace*{3em}
		\includegraphics[height=2in]{soft_param_sharing.pdf}
		\subcaption{Soft paramter sharing}
	\end{subfigure}
	\caption{Multi-task learning}
	\label{fig:multiLearning}
\end{figure}
Humans do not learn things in isolation. Instead, we use knowledge gained from prior experience to help learn later tasks. Multi-task learning takes inspiration from this axiom: models are trained on multiple related tasks with the aim of improving generalization \cite{zhangSurveyMultiTaskLearning2017}. Deep neural networks generally need very large training sets, but there are many contexts where large labelled training sets are difficult to come by (e.g. medical image labelling) so instead, several smaller labelled datasets from similar tasks can be used \cite{zhangSurveyMultiTaskLearning2017}. More generally however, multi-task learning can useful because learning only a single task may be restrictive and ignores useful information that could be learned from other tasks to improve performance in the original task \cite{ruderOverviewMultiTaskLearning2017}. Multi-task learning may thought of as inductive transfer: by sharing representations among tasks an inductive bias is introduced, leading to the model preferring some hypothesis over others \cite{ruderOverviewMultiTaskLearning2017}. There are two main approaches to multi-task learning: hard parameter sharing and soft parameter sharing.	Hard parameter sharing is used in multi-task neural networks, and involves sharing hidden layers between all tasks, while using separate output layers for each task (see Figure \ref{fig:multiLearning}a) \cite{ruderOverviewMultiTaskLearning2017}. In soft parameter sharing seperate models are trained for each task, but the distance between parameters across models is regularized to encourage similarity between models (see Figure \ref{fig:multiLearning}b) \cite{ruderOverviewMultiTaskLearning2017}.
\section{Features and Embeddings}\label{sec:featsEmbeds}
In text classification we face the problem of how best to represent the input data, since machine learning models generally require natural numbers as inputs. In this section we review two common representation methods:  Bag of words and word embeddings.

\subsection{Bag of Words Model and Feature Functions}\label{sec:vecSpace}


\begin{table}[]
	\caption{Simple Bag-of-Words example using term frequency weighting}
	\label{bagOfWords}
	\centering
	\begin{tabular}{@{}lllllllll@{}}
		\toprule
		& and                    & cat                    & eggs                   & green                  & ham                    & hat                    & in                     & the                    \\ \midrule  \cline{2-9} 
		\multicolumn{1}{l|}{`the cat in the hat'} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2} \\ \cline{2-9} 
		\multicolumn{1}{l|}{`green eggs and ham'} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} \\  \cline{2-9} 
		&                        &                        &                        &                        &                        &                        &                        &                        \\ \bottomrule
	\end{tabular}
	
\end{table}

A common method for representation in information retrieval and natural language processing is the vector space model as proposed by Salton, Wong and Yang \cite{saltonVectorSpaceModel1975}. In their vector space model a document in a collection $ D_i $ is represented by a $ t $ dimensional vector of term weights: $$ D_i = (d_{i1},\ d_{i2},\ \dots,\ d_{ij})  $$ where $ t $ is the magnitude of the vocabulary present in the document collection and $ d_{ij} $ is the weight value for the $ j^{th} $ term in the $ i^{th} $ document \cite{saltonVectorSpaceModel1975}.
The order of words is not considered, only the presence and the frequency. This is known as the Bag-of-Words model (see Table \ref{bagOfWords}) \cite{jurafskySpeechLanguageProcessing}. The term weight value is determined by a weighting scheme. Common weighting schemes are binary, term-frequency and tf-idf.

In binary term weighting each term weight $ d_{ij} $ is a binary indicator taking the value of either 0 or 1 to indicate the presence of the $ j^{th} $ term. Term frequency is ignored. This is analogous to One-Hot-Encoding in which categorical variables such as gender are converted to real numbers as a vector of binary indicators, with one indicator per variable category \cite{geronHandsonMachineLearning2017}.

In term-frequency weighting each term weight is determined as $d_{ij} = tf_{ij}$ where $tf_{ij}$ is the frequency of the $ j^{th}$ term in the $i^{th}$ document. However, this introduces a large bias towards frequent terms such as \{\textit{the, I, you}\} which often encode little to no semantic information \cite{manningIntroductionInformationRetrieval2009}.

Tf-idf avoids this bias by weighting each term frequency by the \textit{inverse document frequency} which is defined by Manning et al.\ \cite{manningIntroductionInformationRetrieval2009} as:

$$idf_j = log \frac{N}{df_j} $$

Where $ N $ is the number of documents in the collection and $ df_j $ is the number of documents that contain the $ j^{th} $ term. Hence, we can see that the tf-idf weight of common terms will be attenuated by a low idf, and conversely rare terms (which encode greater semantic information) will have their weights boosted by a high idf \cite{manningIntroductionInformationRetrieval2009}.

Here by examining the vector space model we have conveniently introduced the concept of a feature function that produces a feature vector. Often denoted by $ \phi() $ or $ \phi(\mathbf{x}) $,  with careful selection and tuning the feature selection process can help to increase the discriminating power of our classifier \cite{bishopPatternRecognitionMachine2006}. Bishop \cite{bishopPatternRecognitionMachine2006} highlights that we can take advantage of domain specific knowledge to develop effective feature functions. For example in text classification we might wish to use a feature function that concatenates the Bag-of-Words representation with other semantic or syntactic features under the intuition that such extra information might increase classifier performance. 

\subsection{Vector Semantics and Word Embeddings}\label{sec:vecSeman}
\begin{figure}[h]
	\centering
	\includegraphics[width=4in]{word_embeddings.pdf}
	\caption{Visualisation of word embeddings in 2-D
		\label{fig:wordEmbeds} space}
\end{figure}

Section \ref{sec:vecSpace} briefly introduced the vector space model and highlights the importance of feature extraction for textual data. While simple N-gram bag of words models are conceptually simple and generally effective, such approaches suffer from two important problems. Firstly, words are treated as atomic units and no semantic similarity is captured in the feature representation. Secondly, when using large training vocabularies feature vectors become very sparse: in an N-gram model feature dimensionality scales as $ |V|^N $ where $ N $ is gram length and $ |V|  $ is the size of vocabulary.  The complete Oxford English Dictionary alone contains nearly 250,000 words \cite{HowManyWords} without taking account of technical terms, dialects and regional spelling variation, so it is clear that any suitably large training corpus will result in sparse representations \cite{jurafskySpeechLanguageProcessing}. The limitations of such sparse feature representations can however be avoided by using vector semantics and dense word embeddings \cite{jurafskySpeechLanguageProcessing}.

The distributional hypothesis  posits that words appearing in similar contexts have similar meanings \cite{sahlgren2008distributional}. Vector semantics aims to capture this similarity by representing words as dense vectors embedded in a multidimensional space, which are learned from the distributions of words and contexts in corpora \cite{jurafskySpeechLanguageProcessing}. This is known as representation learning, where we generate representations of text by unsupervised learning instead of feature engineering \cite{jurafskySpeechLanguageProcessing}. Figure \ref{fig:wordEmbeds} shows an extract of what a word vector embedding space might look like projected into two dimensions. Notice that semantically similar words are spatially grouped. There are several popular formats for word embeddings including skip-gram (word2vec) \cite{mikolovEfficientEstimationWord2013}, GloVe \cite{penningtonGloveGlobalVectors2014} and fastText \cite{joulinBagTricksEfficient2016}, though there are broad similarities between all three methods. A key innovation in this field was the use of running text as implicitly supervised training data - any given sentence already contains the target variable for a classifier that predicts a word in context \cite{jurafskySpeechLanguageProcessing}. In particular, a neural language model that accomplishes this will learn a weight matrix from which embedded word representations can be extracted \cite{jurafskySpeechLanguageProcessing}. 


\section{Anchors}\label{sec:anchors}

\begin{figure}
	\centering
	\includegraphics[width=4in]{Anchors.pdf}
	\caption{Simple example of anchors and perturbation distribution}
	\label{fig:Anchors}
\end{figure}
While advanced neural network models such as RNNs and LSTMs can offer high accuracy in complex tasks, a particular drawback is that their sophistication results in a black-box effect. Ribeiro et al.\ \cite{ribeiroAnchorsHighPrecision} highlights that interpretability is an important quality in machine learning. Defining \textit{human precision} as the fraction of instances where a human could confidently and correctly predict what the model will predict, they argue that for a model to be truly interpretable, human precision must be high \cite{ribeiroAnchorsHighPrecision}. Riberio et al.\ thus introduce a novel model agnostic method for interpreting the behaviour of complex black box models using explanations known as \textit{anchors}. Given a black-box model $ f: X \rightarrow Y $, the system uses a `perturbation distribution' $ D_x $ for instance $ x$ to examine how the model prediction $ f(x) $ changes locally under perturbation. In the context of text classification, the perturbation involves substituting tokens for random words with same POS tag and probability proportional to their similarity in embedding space \cite{ribeiroAnchorsHighPrecision}. An `anchor' is a rule (set of feature predicates) that is a sufficient condition for $ f(x) $ with a high probability across the conditional perturbation distribution, i.e. it is highly probable that $f(z)=f(x)  $ for a random sample $ z $ from $ D_x(z|A) $. Formally, a rule $ A $ is an anchor if:
\[ \mathbb{E}_{D(z|A)}(\delta_{f(z),f(x)}) \geq \tau, A(x)=1 \]

Where $ \tau $ is a precision parameter and: \[  \delta_{f(z),f(x)} =\begin{cases}
1 & \text{if}\ f(z)=f(x)\\
0 &  \text{otherwise}
\end{cases} 	\]

Figure \ref{fig:Anchors} shows a simple example of an anchor that might be found in the case of sentence sentiment analysis for product reviews. In this example $ x = \text{`\textit{The phone is not bad}'} $ and an anchor $ A=\{\textit{`not', `bad'}\} $ is identified. We see a subset of the perturbation distribution $ D_x $ and a subset of the conditional perturbation distribution $ D_x(\cdot|A) $. In practice, identifying all perturbations and anchors is computationally infeasible, and so Ribeiro et al. adopt a beam search algorithm to find anchors \cite{ribeiroAnchorsHighPrecision}.
\section{Speaker Stance in Linguistics}\label{sec:speakerStanceLing}
Before examining the stance detection literature, the notion of speaker stance must be conceptualised and explored to broadly inform the remainder of this project. There exists a wealth of contemporary linguistic research pertaining to stance. A number of textbooks and monographs have been published explicitly devoted to stance and stancetaking
\cite{hunstonEvaluationTextAuthorial2000}, \cite{englebretsonStancetakingDiscourseSubjectivity2007}, \cite{karkkainenEpistemicStanceEnglish2003}, 
%
% consider revising below
conferences have been hosted to bring together researchers in the field
%
%
and  many journal articles from different subfields in linguistics have converged upon the topic \cite{biberStylesStanceEnglish1989}, \cite{wangCorpusbasedStudyStancetaking2018}, \cite{myersImpersonalYouStancetaking2012}. This heterogeneous body of research demonstrates a marked interest in developing understanding of stance, however it also demonstrates that stance is a broad and nuanced topic, with no universally agreed upon definition in academia. This section closely follows the work of Englebretson \cite{englebretsonStancetakingDiscourseSubjectivity2007} in order to present an overview of linguistic perspectives on stance.

To begin with, it is useful to examine the colloquial usage of the term \textit{stance} in natural language, since this can offer insight into the ways in which its meaning has been appropriated by the linguistic research community \cite{englebretsonStancetakingDiscourseSubjectivity2007}. This approach is known as a usage-based perspective of language, which asserts that language form and meaning can be best understood by examining language use, as opposed to understanding language through rule based systems \cite{barlowUsagebasedModelsLanguage2000}. 

The Oxford Dictionary of English  \cite{StanceOxfordReference} defines the noun stance to mean:

\begin{displayquote}
	\begin{enumerate}
		\item the way in which someone stands, especially when deliberately adopted.
		\item the attitude of a person or organization towards something; a standpoint.
	\end{enumerate}
\end{displayquote}

This definition provides a useful starting point. Note that stance is defined as referring either to a physical orientation or a subjective outlook - this is the foundation of most linguistic characterizations of stance. Investigating in more detail the properties of the term and the contexts in which it appears further assists the construction of academic definitions of speaker stance. Englebreston \cite{englebretsonStancetakingDiscourseSubjectivity2007} presents a corpus based quantitative and qualitative analysis of the usage of the term stance in colloquial language. The analysis identifies five key principles that describe stance, which we will examine in detail in this section. Consider the following sentences, inspired by entries examined in the corpus analysis \cite{englebretsonStancetakingDiscourseSubjectivity2007}:


\begin{enumerate}
	\renewcommand{\labelenumi}{(\Alph{enumi})}
	\item `The fighter took a defensive stance'
	\item `He was known amongst his peers for his conservative political stance'
	\item `Young people are leaving the Catholic Church due to its moral stance on abortion'
	\
\end{enumerate}

The five key principles identified by Englebretson \cite{englebretsonStancetakingDiscourseSubjectivity2007} are:
\begin{enumerate}
	\item Stancetaking can occur in three overlapping ways - as a physical action, a personal attitude or a social value. (A) clearly demonstrates the physical sense of \textit{stance}, while (B) shows it as a personal attitude, and in (C) we see it can indicate a social value (morality). 
	
	\item Stance is public, interpretable and available to for inspection by others. For example, in (A) the fighter's opponent can notice the defensive stance, while in example (B) the subject's peers have assessed his political stance to be a key aspect of his character.
	
	\item Stance is interactional in nature - it cannot exist in a vacuum. In (A) the  phrase \textit{defensive stance} refers to the fighters positioning in against his opponent, while in (C) the Church's stance is in opposition of those in favour of abortion. Without the notional `other', the stances do not make sense.
	
	\item Stance is indexical, i.e. the context in which \textit{stance} is used can point towards other unmentioned attributes. For example, in (B) the conservative political stance implicitly 'indexes' the subject as holding certain views  associated with conservatism, and also implies that his peers do not. Similarly, in (C) the categorization of their stance as `moral' implicitly suggests that the Church disagrees with abortion.
	
	\item Stance is consequential. In (A) the fighter may find it harder to attack his opponent due to his defensive stance, while in (B) the subject has been labelled by his peers for his stance, and (C) explicitly states a consequence of the Church's stance on abortion.
\end{enumerate}

Engelbretson \cite{englebretsonStancetakingDiscourseSubjectivity2007} further justifies these five principles by quantitative analysis of the corpora. A key observation is that the term \textit{stance} is a relatively rare lexeme generally that appears far more frequently in written text than in spoken language. This suggests that \textit{stance} is a term that appears mostly in specialized contexts. These contexts are demonstrated by examining the adjective collocates of \textit{stance} \cite{englebretsonStancetakingDiscourseSubjectivity2007}. The five most common adjectives thereby are \textit{political, aggressive, moral, upright} and \textit{tough}. These five adjectives broadly speaking support the aforementioned five principles. For example, \textit{political} and \textit{moral} exemplify stance as a personal attitude or social value, while \textit{upright} demonstrates the physicality of stance. Finally, \textit{tough} and \textit{aggressive} can be interpreted in all three contexts.
%######################################
% CONTINUE OR RAP UP

%%Consider extending quantitative justifications

Having considered qualitative and quantitative perspectives on the usage of stance we can see that the term has many contexts and definitions.  It is therefore consistent that the appropriation into academic contexts of stance comes in many forms, with different researchers invoking varying practical definitions of speaker stance.

Critical to understanding such definitions of stance are \textit{subjectivity} and \textit{evaluation}. Subjectivity is the aspect of language that allows interlocutors (speakers)  to express themselves and describe their own point of view, for example using the pronouns \textit{I} and \textit{Me} \cite{matthewsSubjectivity2014}. Evaluation is the expression of subjective sentiment concerning an entity or proposition, such as expressing an attitude or opinion \cite{hunstonEvaluationTextAuthorial2000}. These two concepts are relied upon heavily by Biber and Finegan \cite{biberStylesStanceEnglish1989}, which defines stance as:

\begin{displayquote} `` The lexical and grammatical expression of attitudes, feelings, judgments, or commitment concerning the propositional content of a message ''
\end{displayquote}


Furthermore, Biber and Finegan \cite{biberStylesStanceEnglish1989} identifies twelve groups of stance markers (such as certainty verbs or predictive modals) based on semantic and grammatical criteria. Using cluster analysis of corpora using these stance markers, they identify six separate styles of stance, such as ‘Emphatic Expression of Affect’ and ‘Expository Expression of Doubt’ \cite{biberStylesStanceEnglish1989}. 

In a similar fashion Simaki et al.\ \cite{simakiAnnotatingSpeakerStance2017} (which this project is based upon and is explored further in \ref{BBC}) introduces a frame work for labelling the stance of an utterance according to ten notional categories based on the definition of stance put forth by Du Bois \cite{duboisStanceTriangle2007}:

\begin{displayquote}
	`` One of the most important things we do with words is to take a stance. Stance has the power to assign value to objects of interest, to position social actors with respect to those objects, to calibrate alignment between stance takes and to invoke systems of socio-cultural value. ''
\end{displayquote}

DuBois' \cite{duboisStanceTriangle2007} definition highlights the importance of stancetaking. This is supported by Stubbs  which argues that subjective attitudes and viewpoints are encoded in all forms of language \cite{stubbsMatterProlongedField1986}. Furthermore, Stubbs suggests such is the prevalence of stancetaking that researching and describing markers thereof ``should be  a  central
topic for linguistics'' \cite{stubbsMatterProlongedField1986}.

To summarise, there are great many ways to consider the notion of speaker stance from a linguistic point of view. Considering the usage and meanings of stance in natural language provides insight into how academic use of the term has stemmed from the colloquial definition \cite{barlowUsagebasedModelsLanguage2000}. Qualitative and Quantitative analysis of corpora allows the formalisation of five key principles of stance which help to understand academic definitions of speaker stance \cite{englebretsonStancetakingDiscourseSubjectivity2007}. In turn, the study of speaker stance in its varying forms introduces frameworks to formalise the classification of speaker stance categories and styles \cite{biberStylesStanceEnglish1989}, \cite{simakiAnnotatingSpeakerStance2017}. The framework for stance labelling by Simaki et al.\ \cite{simakiAnnotatingSpeakerStance2017} provides ten categories that will be explored in further detail in section \ref{BBC}. The framework is introduced concurrently with a dataset for exploring the task of stance detection and classification \cite{simakiAnnotatingSpeakerStance2017}. This task and the dataset provided form the central theme of this project: developing machine learning models for detecting speaker stance in the Brexit Blog Corpus \cite{simakiAnnotatingSpeakerStance2017}.

\section{Previous Research in Stance Detection} \label{sec:priorWork}
In recent years there has been a growing interest in stance detection. In this section we will review some of the prior research in the field. Within the literature there are many similarities between papers in terms of the data used and the approaches taken to stance detection. There are also significant differences among papers, so it is important to compare and contrast approaches to ensure the remainder of this project is well informed.

\subsection{Context and Related Tasks}
Stance detection is often grouped with other natural language processing tasks. For example, the Fake News Challenge suggests stance detection as a preliminary step in fake news identification, while Ferreira and Vlachos consider stance detection as part of a wider set of natural language processing tasks used for fact-checking \cite{ferreiraEmergentNovelDataset2016}. Similarly, Respall and Derczynski \cite{respallStanceDetectionCatalan} considers stance detection together with author gender identification.

\subsection{Datasets and Topics}
 Stance detection inherently concerns opinionated content and as such datasets presented for the task usually focus on online debate, micro-blogging and journalism. While this kind of content is very prevalent online, manual annotation is burdensome. Additionally, stance detection remains a fairly novel niche within natural language processing and so the variety of public datasets is limited. Two shared tasks form most the commonly used datasets. Firstly, the annotated newswire data from The Fake News Challenge (FNC), of which a retrospective summary is provided by Hanselowski et al.\ \cite{hanselowskiRetrospectiveAnalysisFake2018} and secondly the annotated twitter data from SemEval Task 6: Detecting Stance in Tweets (hereafter, SemEval), as summarised by Mohammad et al.\ \cite{mohammadSemEval2016TaskDetecting2016}. Another common approach is to create a task specific dataset by scraping twitter data and annotate it using hashtag data as indicators of stance \cite{swamiEnglishHindiCodeMixedCorpus2018}.  Ferriera and Vlachos \cite{ferreiraEmergentNovelDataset2016} present Emergent, a novel dataset of news article headlines labelled according to the stance they take with respect to a collection of target claims. There also exist several older datasets created from online debate forums such as Somasundaran and Wiebe \cite{somasundaranRecognizingStancesIdeological2010}, and Hasan and Ng \cite{hasanExtraLinguisticConstraintsStance} Going back further still. Finally, Simaki et al.\ \cite{simakiEvaluatingStanceannotatedSentences2018} undertakes stance classification using the Brexit Blog Corpus, 
 
\subsection{Languages}
Given that stance detection almost invariably uses datasets created from online content, and given that English is the most common language online \cite{TopTenInternet} previous work mostly focusses on detecting stance in English language text \cite{ferreiraEmergentNovelDataset2016,hanselowskiRetrospectiveAnalysisFake2018,mohammadSemEval2016TaskDetecting2016,somasundaranRecognizingStancesIdeological2010,hasanExtraLinguisticConstraintsStance,augensteinStanceDetectionBidirectional2016,sobhaniStanceDetectionAnalysis}. However, work in other languages has been published. For example, Swami et. al \cite{swamiEnglishHindiCodeMixedCorpus2018} uses dataset of code-mixed\footnote{The mixing of two or more languages in speech or text by multilingual interlocutors} Hindi-English tweets. Du et al.\  \cite{duStanceClassificationTargetspecific} demonstrate a language independent approach evaluated with both an English twitter dataset and a Chinese Weibo\footnote{Chinese micro-blogging platform similar to Twitter} dataset. Finally, Respall and Derczynski \cite{respallStanceDetectionCatalan} use a dataset consisting of Spanish and Catalan tweets annotated again in the tertiary labelling scheme.

 
\subsection{Classification Framework}
With the exception of the Brexit Blog Corpus, the literature overwhelmingly shows stance detection as a single label classification scheme \cite{ferreiraEmergentNovelDataset2016,respallStanceDetectionCatalan,hanselowskiRetrospectiveAnalysisFake2018,mohammadSemEval2016TaskDetecting2016,swamiEnglishHindiCodeMixedCorpus2018,somasundaranRecognizingStancesIdeological2010,hasanExtraLinguisticConstraintsStance,duStanceClassificationTargetspecific,augensteinStanceDetectionBidirectional2016,sobhaniStanceDetectionAnalysis}. Most work considers a three class output space (\textit{in favour}, \textit{neutral}, and \textit{against})\cite{ferreiraEmergentNovelDataset2016,respallStanceDetectionCatalan,mohammadSemEval2016TaskDetecting2016,swamiEnglishHindiCodeMixedCorpus2018,somasundaranRecognizingStancesIdeological2010,hasanExtraLinguisticConstraintsStance,duStanceClassificationTargetspecific,augensteinStanceDetectionBidirectional2016,sobhaniStanceDetectionAnalysis}, with Ferreira and Vlachos using the three classes \textit{in favour}, \textit{observing} and \textit{against} \cite{ferreiraEmergentNovelDataset2016}. The Fake News Challenge introduces a fourth class, using \textit{Agree}, \textit{Discuss}, \textit{Disagree} and \textit{Unrelated}. In contrast, The Brexit Blog Corpus a multi-label framework whereby instances may be labelled with up to five labels from a set of ten stance categories \cite{simakiAnnotatingSpeakerStance2017}. This allows the labelling for each instance to be much more informative and expressive, however it also creates a much more complex output space and represents a much more challenging task for stance detection.
\subsection{Methods and Results}

\begin{table}[]
	\caption{Previous stance detection work ranked by \f{Macro} score}
	\label{tab:reviewResults}
	\begin{tabularx}{\textwidth}{p{5cm}Xp{2.5cm}p{1cm}}
		\toprule
		Author/Team          & Classifier                               & Dataset             & \f{Macro}  \\ \midrule
		Du et al. \cite{duStanceClassificationTargetspecific}            & TAN                                      & Weibo               & 72.88                            \\
		Ferreira and Vlachos \cite{ferreiraEmergentNovelDataset2016} & Logistic Regression                      & Emergent            & 72.32                            \\
		Dey et al.\cite{deyTwitterStanceDetection2017}           & 2 Phase SVM                              & SemEval A      & 69.53                            \\
		Dey et al. \cite{deyTopicalStanceDetection2018}           & 2 Phase attention LSTM                   & SemEval A      & 68.84                            \\
		Du et al. \cite{duStanceClassificationTargetspecific}            & TAN                                      & SemEval A      & 68.79                            \\
		MITRE \cite{zarrellaMITRESemEval2016Task2016}                & Stacked RNNs                             & SemEval A      & 67.82                            \\
		Athene \cite{hanselowskiRetrospectiveAnalysisFake2018}               & MLP                                      & FC & 60.40                            \\
		UCLMR \cite{riedelSimpleToughtobeatBaseline2017}                & MLP                                      & FNC & 58.30                            \\
		Talos \cite{largentTalosTargetsDisinformation}                & CNN, GBT ensemble                        & FNC & 58.20                            \\
		Augenstein et al. \cite{augensteinStanceDetectionBidirectional2016}    & BiCond LSTM & SemEval B      & 58.03                            \\
		pkudblab \cite{weiPkudblabSemEval2016Task2016}           & Rule augmented CNN                       & SemEval B      & 56.28                            \\ \bottomrule
	\end{tabularx}
\end{table}

In terms of the approaches taken to stance detection, the literature is split between feature engineering approaches and neural, representation learning approaches. Below these methods are outlined to provide context for this project in terms of what the current state of the art includes. The results of the papers reviewed are summarised in Table \ref{tab:reviewResults}.

Mohammad et al.\ \cite{mohammadSemEval2016TaskDetecting2016} provides a retrospective summary of SemEval \cite{TaskDetectingStance} . Within SemEval \cite{TaskDetectingStance} were two subtasks, task A and task B. The highest ranking submission for task A,\textit{ MITRE}, developed a semi-supervised approach stacking two RNNs, achieving an overall macro averaged F-score of 67.82 \cite{mohammadSemEval2016TaskDetecting2016}, \cite{zarrellaMITRESemEval2016Task2016}. However, this did not beat the n-gram Support Vector Machine (SVM) Baseline created by the task organisers. The winning system for task B, \textit{pkudblab}, used a weakly supervised approach that used a rule based annotation of the unlabelled training data to augmented a deep convolutional network, achieving an overall macro averaged F-score of 56.28 \cite{mohammadSemEval2016TaskDetecting2016}, \cite{weiPkudblabSemEval2016Task2016}.

Augenstein et al.\ \cite{augensteinStanceDetectionBidirectional2016} publishes a post-hoc method for SemEval task B \cite{TaskDetectingStance}, developing a method known as bi-directional conditional encoding. Encoding refers to generating separate representations of tweets and target topics using two LSTMs \cite{augensteinStanceDetectionBidirectional2016}\cite{hochreiterLongShorttermMemory1997}. Conditional encoding improves upon this by initializing the tweet LSTM with the state of the target LSTM, thereby generating a target dependent representation of each tweet \cite{augensteinStanceDetectionBidirectional2016}. Finally, Bi-directional means that the conditional encoding process is performed reading the text both left to right and right to left to enhance the representation and improve performance \cite{augensteinStanceDetectionBidirectional2016}. The system results in a macro averaged F-score 58.03 and uses a weakly supervised approach.

Dey et al.\ \cite{deyTwitterStanceDetection2017} provides another post-hoc approach to SemEval, but for task A. The method uses feature engineering and a two phase SVM classifier. The first phase seperates neutral tweets from non-neutral using WordNet \cite{WordNetLexicalDatabase} features and a subjectivity-polarity score from the MPQA subjectivity lexicon \cite{SubjectivityLexiconMPQA}. The second phase then differentiates between Favour and Against tweets using MPQA and SentiWordNet \cite{SentiWordNet} features, character and word n-grams, frame semantics and boolean indicators for the presence of target words in tweets. This approach nets a macro averaged F-score of 69.53, and thus represents the current state of the art with respect to SemEval task A.

Du et al.\ \cite{duStanceClassificationTargetspecific} introduces the \textit{Target-specific Attention Neural Net-
work (TAN)}, a novel application of neural attention networks for the purpose of stance detection. In the neural attention network an attention signal is learned. The signal is multiplied with instance features effectively highlighting critical elements of text in relation to a target topic and masking unimportant elements. This mechanism is incorporated into a bi-directional RNN that achieves a macro averaged F-score of 68.79 on SemEval task A, and 72.88 on a Chinese dataset.

Dey et al.\ return to the SemEval problem in \cite{deyTopicalStanceDetection2018}, this time publishing a two phase LSTM method that incorporates the attention mechanism of \cite{duStanceClassificationTargetspecific}. This method achieves a macro averaged F-score of 68.84 on SemEval, currently representing the state of the art for this task.

The work of Ferriera and Vlachos \cite{ferreiraEmergentNovelDataset2016} on the Emergent datasets uses a logistic regression classifier and a feature space combining simple headline linguistic features such as Bag-of-words and punctuation with headline-claim features such as alignment, entailment and cosine similarity, achieving a test set accuracy of 73\%. The precision and recall figures published in Ferreira and Vlachos correspond to a macro averaged F-score of 72.31

Hanselowski et al. \cite{hanselowskiRetrospectiveAnalysisFake2018} provides a retrospective analysis of the Fake News Challenge. The review considers the three most highly ranking submissions from the shared task \textit{Athene},\textit{Talos},  and \textit{UCLMR}. Athene submitted a multi-layer perceptron with six hidden layers and using engineered features, resulting in an \f{Macro} score of 60.4 \cite{hanselowskiRetrospectiveAnalysisFake2018}. UCLMR also uses a multi-layer perceptron, however with only a single hidden layer and using Tf-idf vectors as features \cite{riedelSimpleToughtobeatBaseline2017}, \cite{hanselowskiRetrospectiveAnalysisFake2018}.Their approach netted a \f{Macro} score of 58.3. Finally, Talos used a weighted average ensemble model of a deep convolutional model and gradient boosted tree model, achieving an \f{Macro} of 58.2 \cite{largentTalosTargetsDisinformation}, \cite{hanselowskiRetrospectiveAnalysisFake2018}. 

To summarise, while different datasets are used across the systems discussed, they are largely similar tasks and so comparison between classifiers is still of use. Several conclusions can be reached from the comparison of these methods and their results. Many approaches to stance detection use complex neural networks that take advantage of recurrent structures and use representation learning or pre-trained word embeddings  \cite{augensteinStanceDetectionBidirectional2016,duStanceClassificationTargetspecific, deyTopicalStanceDetection2018,zarrellaMITRESemEval2016Task2016,largentTalosTargetsDisinformation,weiPkudblabSemEval2016Task2016}.  The performance of these models is generally high, often being the state of the art for the problem at hand. Secondly, though complex recurrent models are popular, they cans still be outperformed by more conventional methods when combined with careful feature engineering. For instance, the logistic regression classifier implemented by Ferreira and Vlachos achieves the second highest \f{Macro} score of the systems surveyed, and the third highest score is posted by the 2 phase SVM of Dey et. al \cite{deyTwitterStanceDetection2017}. Similarly, multi-layer perceptrons are also utilised in the literature, and when the architecture is carefully designed they perform  strongly. These observations provide context as to the state of the art in stance detection, but also act as a starting point for the design of this project. In particular, this project aims investigate achieving state of the art performance with efficient, compact learning methods and as such the observation that complex recurrent networks can be outperformed by carefully tuned linear methods is especially pertinent.

\section{The Brexit Blog Corpus}\label{BBC}

Having explored definitions of speaker stance and reviewed the current state of stance detection, the remainder of the literature review focusses on the principle resource for this project. Simaki et al.\ \cite{simakiAnnotatingSpeakerStance2017} introduced The Brexit Blog Corpus, a novel dataset annotated for speaker stance. The work is undertaken in the field of corpus linguistics and linguistic theory, and therefore uses a much richer operational definition of stance (as outlined in section \ref{sec:speakerStanceLing}) compared to work published in machine learning journals. Simaki et al.\ \cite{simakiAnnotatingSpeakerStance2017} acknowledges this, justifying that their approach offers much more context sensitivity than traditional simple lexically driven approaches. In section \ref{sec:priorWork} it is noted that most work considers stance detection as a single-label, tertiary classification task, however \cite{simakiAnnotatingSpeakerStance2017} introduces a labelling scheme comprising ten stance categories as per Table \ref{tab:stanceCats}. The dataset presents stance annotation as a multi-label classification problem, in contrast to previous work which only considers single-label classification schemes. 

{\renewcommand{\arraystretch}{1.5}
	
	\begin{table}[h]
		\caption{Stance categories used in annotation of the Brexit Blog corpus \cite{simakiAnnotatingSpeakerStance2017}}
		\label{tab:stanceCats}
		\small\scshape 
		\centering
		\begin{tabular}{|l|l|}
			\hline
			Agreement/Disagreement & Certainty       \\ \hline
			Contrariety            & Hypotheticality \\ \hline
			Neccessity             & Prediction      \\ \hline
			Source of Knowledge    & Tact/Rudeness   \\ \hline
			Uncertainty            & Volition        \\ \hline
		\end{tabular}
		
\end{table}}

This broader interpretation of \textit{stance} offers significant novelty and potential for natural language procession applications, however it also presents a much more complex problem space. At the time of writing, there is only a single paper published using the Brexit Blog Corpus for stance detection, by Simaki et al. \cite{simakiStanceClassificationTexts2017}. This serves as a preliminary baseline for the project, and therefore an aim of this project to build on the work of Simaki et al., develop more accurate classifiers and further research into stance classification using the Brexit Blog Corpus.

\subsection{Quantitative Linguistic Analysis}\label{sec:quantAnalysis}
Simaki et al.\ \cite{simakiEvaluatingStanceannotatedSentences2018} examines a subset of the Brexit Blog Corpus for the purpose of formal quantitative analysis of linguistic characteristics. The subset comprises six out of the ten categories, and for each category mean frequency values are obtained for 29 linguistic features such spaces, upper case letters etc. Simaki et al.\cite{simakiEvaluatingStanceannotatedSentences2018} then uses one-way Analysis of Variance (ANOVA) to determine if there are significant differences between group means for each feature. The null hypothesis ($ H_0 $) was rejected if two or more means were significantly different at a 5\% significance level, corresponding to a critical F-value of $ F_{crit} =2.220 $. This test establishes which features exhibit significant differences in frequency between categories and could therefore prove useful in discriminating between stance categories. The results of this analysis are reviewed in further detail in Section \ref{sec:featEngin}

\subsection{Stance Detection Using The Brexit Blog Corpus}
Simaki et al. \cite{simakiStanceClassificationTexts2017} presents the first implementation for classifying stance based on the cognitive functional framework of ten notional speaker stance categories set out in Simaki et al. \cite{simakiAnnotatingSpeakerStance2017}. A key aim of the study was to determine whether the stance categories set out in their framework can be automatically identified baseline classifiers and features. The approach notes the imbalance within the Brexit Blog Corpus, opting to use a subset the data that excludes the four rarest categories \lab{agreement/disagreement, certainty, tact/rudeness},  and \lab{volition}. Two approaches are considered in the paper, using label transformations as outlined in  Section \ref{sec:multiLabel} and Table \ref{miniData}. Firstly, using a copy transformation and training multi-class classifiers and secondly, by using the RPC transformation and training a classifier on each of the fifteen stance category pair datasets. 

Both models use simple baseline features. In particular, the features were either syntactic (POS tags etc.) or lexical (character frequency, word length etc.)

In the copy transform implementation a ten different classifier types were evaluated, achieving test accuracies between 20.95\% and 30.00\%. The classifiers included logistic regression and a multi-layer perceptron, the test results for which are show in Table \ref{tab:simakiMLPLR}. The logistic regression classifier performed best out of the ten classifiers tested, and therefore the categorical \f{1} scores for logistic regression were also recorded. These are shown in table \ref{tab:simakiCateg}. Overall the copy transform method did not produce satisfactory results, and so this provides strong justification for further work such as this project.


\begin{table}
	\centering
	\caption{Results for multi-layer perceptron and logistic regression classifiers in Simaki et al. \cite{simakiStanceClassificationTexts2017}}
	\label{tab:simakiMLPLR}
	\begin{tabular}{lr}
		\toprule
		Classifier 	& 	Accuracy\\ \midrule
		MLP			&	20.95\%\\
		Logistic regression & 30.00\% \\ \bottomrule
		
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{Categorical \f{1} scores for logistic regression classifier in Simaki et al. \cite{simakiStanceClassificationTexts2017}}
	\label{tab:simakiCateg}
	\begin{tabular}{lr}
		\toprule
		Stance category		& \f{1} \\ \midrule
		\lab{Contrariety}	& 0.389 \\
		\lab{Hypotheticality} & 0.135 \\
		\lab{Necessity}		& 0.241 \\
		\lab{Prediction} 	& 0.274 \\
		\lab{Source of Knowledge} & 0.363 \\
		\lab{Uncertainty} 	& 0.049 \\ \bottomrule
	\end{tabular}
\end{table}


The RPC implementation delivered improved results. Table \ref{tab:simakiRPC} shows the results achieved. This implementation faired significantly better than the copy transform approach, achieving a peak test accuracy of 71.17\% for the \lab{Contrariety}-\lab{Necessity} pair. In this format the higher scores correlate with greater differentiability and lower co-occurrence between stance categories, and vice versa.

To summarise, Simaki et al.\ \cite{simakiStanceClassificationTexts2017} puts forth a first attempt at stance classification using the Brexit Blog Corpus. The work employs several of the transformations described in Section \ref{sec:multiLabel}, showing that this is a common strategy for handling multi-label data. The results are mixed, with the copy transform implementation delivering poor results, and the RPC implementation showing improved results. It is therefore the intention of this project to improve upon the work presented in Simaki et al.\ \cite{simakiStanceClassificationTexts2017}.

\begin{table}
	\centering
	\caption{Test accuracy results for RPC implementation of Simaki et al. \cite{simakiStanceClassificationTexts2017}}
	\label{tab:simakiRPC}
	\subfile{../tables/simakiRPC.tex}
	
\end{table}

\begin{comment}
\section{Conclusion}
To summarise, this section has reviewed relevant material that has either informed the direction of this projection or serves as necessary background reading for the topics investigated in this project. Relevant area
\end{comment}

\end{document}